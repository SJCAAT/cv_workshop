{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SJCAAT/cv_workshop/blob/main/workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2148799d",
   "metadata": {},
   "source": [
    "# **Computer Vision Workshop I&D 2025**\n",
    "Welcome to the workshop brought to you by members of the Computer Vision Capability group!\n",
    "\n",
    "In this hands-on session, we'll explore how computers can \"see\" the world using machine learning techniques. You'll get an introduction to Computer Vision, learn how object detection works and use a powerful, real-time model called **YOLOv8 (You Only Look Once version 8)** to detect objects in images and video.\n",
    "\n",
    "In this workshop we will be focussing on two main things:\n",
    "\n",
    "**1) Image classification**\n",
    "\n",
    "Here we implement a pre-trained model to classify images. You will have to import the necessary libraries and ensure the code refers to the correct images to get the model to work.\n",
    "\n",
    "**2) Live detection**\n",
    "\n",
    "After completing the image classification task, we will take it a step further by fine-tuning our model and using this for real-time object detection using our webcams.\n",
    "\n",
    "\n",
    "By the end of this workshop you'll be able to:\n",
    "- Understand basic concepts in computer vision.\n",
    "- Perform object detection with YOLOv8 using the [Ultralytics] library.\n",
    "- Run inference on static images and live webcam feeds.\n",
    "- Explore how to fine-tune YOLOv8 on your own dataset.\n",
    "\n",
    "Whether you're a beginner or have some experience with machine learning, this workshop is designed to be approachable and practical. \n",
    "\n",
    "Lets get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d07417b",
   "metadata": {},
   "source": [
    "# 1) Image Classification\n",
    "First we start with importing all the necessary libraries. Many of these are simply to present images and use the webcam in a notebook format, however the main library we are interested in is Ultralytics. Without this we do not have access to a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7631ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fbcec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image as pil_img\n",
    "from IPython.display import Image, display, Javascript, update_display\n",
    "from google.colab.output import eval_js\n",
    "from google.colab import files # for uploading images to test on\n",
    "from base64 import b64decode\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef84cc26",
   "metadata": {},
   "source": [
    "Now that we have imported all the necessary modules from various libraries we can start with loading a test image. The cell below allows you to upload any image you have stored on your local system. Download any image from the internet and use the code below to upload it.\n",
    "Alternatively, you may choose one of our sample images. We have put up a couple of images in the Github space that you can download to your local machine and use that to upload instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c6c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your own image\n",
    "uploaded = files.upload()\n",
    "image_path = list(uploaded.keys())[0]\n",
    "print(f\"Uploaded image: {image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e58f462",
   "metadata": {},
   "source": [
    "In our import statements we import a function named 'Image' from the PIL library as 'pil_img' which allows us to display images. In order to actually read the image, this function uses the \".open()\" command. This command takes an image as input.\n",
    "\n",
    "In the cell below, implement this function using the 'image_path' that you created in the previous cell.\n",
    "\n",
    "Note: you only need to fill in the first line ('img = .......'). The second command ('img') is then used to display what you just entered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5f2a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the image to a variable\n",
    "img = pil_img.\n",
    "# Display your image\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2afca13",
   "metadata": {},
   "source": [
    "If all went well you should see your test image above. With our test image ready we can use a pre-trained YOLO model to classify objects within that image.\n",
    "\n",
    "We start with choosing the model type. In this session we will be using a pre-trained YOLOv8 model from the ultralytics library. We need to start by initializing the model. Use the cell below as a starting point and complete it appropriately:\n",
    "\n",
    "Hint: You need to load the pre-trained weights for YOLO v8, specifically the 'yolov8n.pt' file using the YOLO() command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e621614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained YOLOv8 model\n",
    "model = 'your_code_here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac80b52",
   "metadata": {},
   "source": [
    "Now that we have seen our image and loaded our pretrained model. Lets use this model to do object recognition on our selected image. \n",
    "In the cell below, apply the model to our chosen image using the 'results' variable.\n",
    "\n",
    "We then immediately visualize the results with the command: 'results[0].show()'\n",
    "\n",
    "Hint: instead of using pil_image.open() on our image_path, we now use model()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebee8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to the image\n",
    "results = 'your_code_here'\n",
    "\n",
    "# Visualize the model predictions\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cec7d5",
   "metadata": {},
   "source": [
    "Hooray! If we got this far it means you've successfully implemented a YOLO model to classify a static image.\n",
    "If there is spare time, feel free to test with some more images that you can find on the web! Remember to change the image_path accordingly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d89755",
   "metadata": {},
   "source": [
    "# 2) Live Detection\n",
    "\n",
    "Now that we've seen the very basics of computer vision, lets take it a step further.\n",
    "\n",
    "In this scenario we are going to attempt to improve the model we just used on our initial images, and prepare it for live detection!\n",
    "\n",
    "In order to fine-tune the model we must have a dataset of images that we can train on. Fortunately for us, plenty of these exist and we can simply use the built-in Coco128 dataset (coco128.yaml). Due to our time constraints we limit the number of epochs (a complete pass through the entire training dataset) to three.\n",
    "\n",
    "Using the same model as before, use the '.train' command on the coco128.yaml dataset for 3 epochs.\n",
    "\n",
    "Hint: the '.train' method is a function that initiates the training process for the model. It can take several arguments but in our case we are only interested in the \"data\" and \"epochs\" arguments.\n",
    " - The 'data' argument takes a string as input\n",
    " - the 'epochs' argument takes an integer as input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3669cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the coco128 dataset\n",
    "model.train(data='', epochs=)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800d3ff9",
   "metadata": {},
   "source": [
    "The beauty of YOLO is that we dont have to search for the best model ourselves. Instead, it automatically stores the best weights it found during training giving us easy access.\n",
    "\n",
    "You can inspect the results of training in the 'runs/detect/train/results.csv'. \n",
    "\n",
    "If you notice that you have a high loss and a low mAP50 this is an indication that training dit not go well and the model may not have actually improved. \n",
    "\n",
    "We assign our 'fine_tuned_model' to use the weights from our training process. The best weights can be found in the 'runs/detect/train/weights/best.pt' folder. \n",
    "\n",
    "If the results look promising, use the cell below to select the best weights so we can apply it for live detection!\n",
    "\n",
    "If you notice a high loss and a low mAP50 and there is enough time, retrain the model and select the weights from the second training run and adjust the path in the cell below accordingly (runs/detect/train2/weights/best.pt). \n",
    "If there is not enough time to retrain, you may simply use the original pre-trained model we used in part 1.\n",
    "\n",
    "Hint: instead of using the pretrained weights like before: YOLO('yolov8n.pt'), we adjust it to include our trained weights instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50715fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best weights found in training\n",
    "fine_tuned_model = YOLO('your_code_here')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06211ae",
   "metadata": {},
   "source": [
    "Now that we (hopefully) have our model fine-tuned, we can use it for live object detection using our webcams!\n",
    "\n",
    "The cells below may look a bit daunting, however we do not expect you to fully understand what is going on below. This is mainly to make sure the webcam works in the notebook version. You may inspect the code if you want but we have filled in everything so that it should work by simply executing the cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9e378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_photo(display_id: str, quality: float = 0.8) -> bytes:\n",
    "    # js snippet to capture frame from webcam\n",
    "    js = Javascript('''\n",
    "        async function takePhoto(quality) {\n",
    "            const div = document.createElement('div')\n",
    "            const video = document.createElement('video')\n",
    "            const stream = await navigator.mediaDevices.getUserMedia({video:true});\n",
    "\n",
    "            div.appendChild(video);\n",
    "            video.srcObject = stream;\n",
    "            await video.play();\n",
    "                    \n",
    "            const canvas = document.createElement('canvas');\n",
    "            canvas.width = video.videoWidth;\n",
    "            canvas.height = video.videoHeight;\n",
    "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "            stream.getVideoTracks()[0].stop();\n",
    "            div.remove();\n",
    "            return canvas.toDataURL('image/jpeg', quality);\n",
    "        }\n",
    "    ''')\n",
    "\n",
    "    # evaluate js and retrieve returned binary image\n",
    "    display(js, display_id=display_id)\n",
    "    data = eval_js('takePhoto({})'.format(quality))\n",
    "    binary = b64decode(data.split(',')[1])\n",
    "    return binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a864e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_image(model: YOLO, binary_img: bytes) -> np.array:\n",
    "    # Run inference on a binary image\n",
    "    img = np.array(pil_img.open(BytesIO(binary_img)))\n",
    "    results = model(img, verbose=False)\n",
    "    return results[0].plot(pil=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ccc4dc",
   "metadata": {},
   "source": [
    "NOTE: If the training did not go well and your fine_tuned_model's performance is not an improvement, simply change the selected model in the cell below to be the pre-trained version we used on our initial image.\n",
    "\n",
    "in line 4 where we see: model = fine_tuned_model, change 'fine_tuned_model' to 'YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8939900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the same display\n",
    "display_id = 'sample_display'\n",
    "# load your fine tuned model. If you are using the pre-trained model without fine tuning - change to: model = YOLO('yolov8n.pt')\n",
    "model = fine_tuned_model \n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # read a frame from the webcam\n",
    "        binary_img = take_photo(display_id='sample_display_2')\n",
    "        # run inference on the frame\n",
    "        result = infer_image(model, binary_img=binary_img)\n",
    "\n",
    "        # show the frame with the inference results\n",
    "        display(result, display_id=display_id)\n",
    "    except Exception as err:\n",
    "        # show error if user does not have a webcam or did not grant page permission\n",
    "        print(str(err))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
